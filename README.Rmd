---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# `baysesp`

<!-- badges: start -->
<!-- badges: end -->

`baysesp` is an `R` package that enables users to estimate the sensitivity and specificity of diagnostic tests while accounting for verification bias using Bayesian methods.

## Installation

You can install the development version of `baysesp` from [GitHub](https://github.com/giovannitinervia9/baysesp) with:

``` r
devtools::install_github("giovannitinervia9/baysesp")
```

## Example

We first generate a dataset using the `baysesp_simul()` function:

```{r, warning=F, message=F}
library(baysesp)

y <- baysesp_simul(n = 1000, p = 0.01, se = 0.9, sp = 0.5)
y
```


Next, we convert the simulated data into the appropriate format for the `baysesp()` function using `y_to_stan_data()`:

```{r}
stan_data <- y_to_stan_data(y)
stan_data
```

We specify our priors as follows:

```{r}
priors <- list(
  p = "beta(0.0199, 1.9701)",
  se = "beta(1, 1)",
  sp = "beta(1, 1)",
  l11 = "beta(15.4, 6.6)",
  l21 = "beta(15, 10)",
  l12 = "beta(4.67, 4.67)",
  l22 = "beta(0.4, 3.6)"
)
```


We now fit the model using `baysesp()`:

```{r, warning=FALSE, message=FALSE, results="hide"}
fit <- baysesp(stan_data, priors = priors, chains = 8, iter = 50000, cores = 8)
```
Let's check the results: 

```{r}
a <- 0.05
s <- summary(fit, probs = c(a / 2, 1 - a / 2))
round(s$summary, 3)
```


